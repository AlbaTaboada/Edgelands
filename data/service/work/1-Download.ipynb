{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb6a4d4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmcmetadata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extract\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediacloud\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trange  \u001b[38;5;66;03m# to display a progress bar\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/mcmetadata/__init__.py:7\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webpages\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m content\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m urls\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m titles\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/mcmetadata/content.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mboilerpy3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extractors \u001b[38;5;28;01mas\u001b[39;00m bp3_extractors\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mreadability\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtrafilatura\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlxml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01metree\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/trafilatura/__init__.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1.4.1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bare_extraction, baseline, extract, html2txt, process_record\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloads\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fetch_url\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extract_metadata\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/trafilatura/core.py:28\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlxml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhtml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tostring\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# own\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m justext_rescue, sanitize_tree, SANITIZED_XPATH, try_readability\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (LANGID_FLAG, check_html_lang, content_fingerprint, duplicate_test,\n\u001b[1;32m     30\u001b[0m                       language_filter, text_chars_test)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhtmlprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (convert_tags, handle_textnode, process_node,\n\u001b[1;32m     32\u001b[0m                              delete_by_link_density, link_density_test_tables,\n\u001b[1;32m     33\u001b[0m                              prune_unwanted_nodes, tree_cleaning)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/trafilatura/external.py:25\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlxml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhtml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m fromstring\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# own\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhtmlprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_tags, prune_unwanted_nodes, tree_cleaning\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreadability_lxml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document \u001b[38;5;28;01mas\u001b[39;00m ReadabilityDocument  \u001b[38;5;66;03m# fork\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msettings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JUSTEXT_LANGUAGES\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/trafilatura/htmlprocessing.py:17\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlxml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01metree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m strip_tags\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlxml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhtml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclean\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Cleaner\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m duplicate_test, textfilter\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msettings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CUT_EMPTY_ELEMS, MANUALLY_CLEANED, MANUALLY_STRIPPED\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trim, uniquify_list\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/trafilatura/filters.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlru\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LRUCache\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msettings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LRU_SIZE\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trim\n\u001b[1;32m     24\u001b[0m LOGGER \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     26\u001b[0m LRU_TEST \u001b[38;5;241m=\u001b[39m LRUCache(maxsize\u001b[38;5;241m=\u001b[39mLRU_SIZE)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/trafilatura/utils.py:28\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     cchardet_detect \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_bytes\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlxml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhtml\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HtmlElement, HTMLParser, fromstring\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# from lxml.html.soupparser import fromstring as fromsoup\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# response types\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/charset_normalizer/__init__.py:23\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mCharset-Normalizer\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m:license: MIT, see LICENSE for more details.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_fp, from_path, from_bytes, normalize\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__, VERSION\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/charset_normalizer/api.py:10\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     PathLike \u001b[38;5;241m=\u001b[39m Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mos.PathLike[str]\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TOO_SMALL_SEQUENCE, TOO_BIG_SEQUENCE, IANA_SUPPORTED\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mess_ratio\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CharsetMatches, CharsetMatch\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn\n",
      "\u001b[0;31mAttributeError\u001b[0m: partially initialized module 'charset_normalizer' has no attribute 'md__mypyc' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from mcmetadata import extract\n",
    "import mediacloud.api\n",
    "\n",
    "from tqdm.notebook import trange  # to display a progress bar\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Init the connection\n",
    "mc = mediacloud.api.DirectoryApi('56196a395ee77c33a296073fa08e72f541362a10')\n",
    "\n",
    "CSV_FILE = 'Query_NYT_13y_surveillance.csv'\n",
    "JSON_FILE = 'data.json'\n",
    "\n",
    "# Read the query.csv file to obtain information of each item to dump\n",
    "df_query = pd.read_csv(CSV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e33b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_query = df_query.shape[0]\n",
    "print(size_query, \"URLs in the CSV file :\", CSV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a263c72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "# Create the result dataframe\n",
    "df_result = pd.DataFrame(columns=[\"year\", \"title\", \"url\", \"text_content\"])\n",
    "\n",
    "# Create the empty JSON file\n",
    "with open(JSON_FILE, \"w\") as f:\n",
    "    f.write(\"[\")\n",
    "\n",
    "with open(\"log.txt\", \"w\") as f:\n",
    "    f.write(\"\")\n",
    "\n",
    "# Création du widget d'erreurs\n",
    "errors_widget = widgets.IntText(description=\"Erreurs sur URL:\", value=0)\n",
    "display(errors_widget)\n",
    "\n",
    "rec = 0\n",
    "\n",
    "for counter in trange(0, size_query):\n",
    "    # Query the MediaCloud API with each item\n",
    "    url_data = df_query.iloc[counter].url\n",
    "\n",
    "    url = \"https://archive.org/wayback/available\"\n",
    "    params = {\n",
    "        \"url\": url_data,\n",
    "        \"timeout\": \"30\",\n",
    "        \"closest\": \"either\",\n",
    "        \"status_code\": \"200\",\n",
    "    }\n",
    "    headers = {\"accept\": \"application/json\"}\n",
    "\n",
    "    response = requests.get(url, params=params, headers=headers)\n",
    "    data = response.json()\n",
    "    closest_snapshot = data.get(\"archived_snapshots\", {}).get(\"closest\")\n",
    "    if closest_snapshot:\n",
    "        closest_url = closest_snapshot.get(\"url\")\n",
    "        is_available = closest_snapshot.get(\"available\")\n",
    "        if is_available == True:\n",
    "            try:\n",
    "                metadata = extract(url=closest_url)\n",
    "                metadata[\"publication_date\"] = metadata[\"publication_date\"].year\n",
    "                metadata[\"url\"] = url_data\n",
    "                metadata[\"original_url\"] = url_data\n",
    "\n",
    "                # Append data to the result dataframe\n",
    "                list_metadata = []\n",
    "                for field in [\n",
    "                    \"publication_date\",\n",
    "                    \"article_title\",\n",
    "                    \"url\",\n",
    "                    \"text_content\",\n",
    "                ]:\n",
    "                    list_metadata.append(metadata[field])\n",
    "                # Append data to the result dataframe\n",
    "                df_result.loc[counter] = list_metadata\n",
    "                # And write data in JSON file\n",
    "                with open(JSON_FILE, \"a\") as f:\n",
    "                    if rec == 1:\n",
    "                        f.write(\",\")\n",
    "                    else:\n",
    "                        rec = 1\n",
    "                    json.dump(metadata, f)\n",
    "                    f.write(\"\\n\")\n",
    "            except Exception as e:\n",
    "                # print(f'Erreur -> \"{e}\", pour l\\'url : {url_data}' )\n",
    "                errors_widget.value += 1\n",
    "                log_message = f\"{datetime.datetime.now()} - {e} ; {url_data}\\n\"\n",
    "                with open(\"log.txt\", \"a\") as log_file:\n",
    "                    log_file.write(log_message)\n",
    "    time.sleep(random.uniform(5.0, 10.0))\n",
    "\n",
    "with open(JSON_FILE, \"a\") as f:\n",
    "    f.write(\"]\")\n",
    "\n",
    "with open(\"log.txt\", \"a\") as log_file:\n",
    "    log_file.write(\"----------------------------------\\n\")\n",
    "print(df_result.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdd5800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export dataframe to a feather format file\n",
    "df_result.reset_index().to_feather('results.feather')\n",
    "\n",
    "print(\"\\n-----------------------------------\")\n",
    "print(\"  Process complete !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
